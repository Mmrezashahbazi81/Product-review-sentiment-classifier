{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ù¾Ø±ÙˆÚ˜Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙˆÛŒ Ú¯Ø±ÙˆÙ‡ Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø´Ù‡Ø¨Ø§Ø²ÛŒ 40119973 Ùˆ ÛŒÚ©ØªØ§ Ø¨ÛŒØ§Øª 40116463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ØªÙˆØ¬Ù‡ : Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú¯Ø±ÙØªÙ† ÙˆÙ‚Øª ØŒ Ø´Ù…Ø§ Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ Ø¨Ø¯ÙˆÙ† Ø±Ø§Ù† Ú¯Ø±ÙØªÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø§ÛŒ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø³ÛŒØ³ØªÙ… Ø¨Ù†Ø¯Ù‡ Ø±Ø§ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø± Ø¨Ø®Ø´ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø³Ù¾Ø³ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ù‚ÛŒÙ‚ ØªØ± Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ù¾Ø³ Ø§Ø² Ù†ØµØ¨ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ Ù‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø¬Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ø¯Ø± Ø¶Ù…Ù† Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ù‡Ø± Ú¯ÙˆÙ†Ù‡ Ø§Ø¨Ù‡Ø§Ù… ÛŒØ§ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø§Ø¬Ø±Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ ØŒ Ø¨Ù†Ø¯Ù‡ Ø¯Ø± Ù‡Ø± Ø³Ø§Ø¹ØªÛŒ Ø¯Ø± ØªÙ„Ú¯Ø±Ø§Ù… Ø¨Ø§ Ø¢ÛŒØ¯ÛŒ Ø²ÛŒØ± Ø¢Ù…Ø§Ø¯Ù‡ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ùˆ Ø±ÙØ¹ Ø§Ø¨Ù‡Ø§Ù… Ù‡Ø³ØªÙ… Ø¨Ø§ ØªØ´Ú©Ø± Ø§Ø² ØªÙˆØ¬Ù‡ Ùˆ ÙˆÙ‚Øª Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ø´Ù…Ø§ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://t.me/Mmreza_81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ù†ØµØ¨ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§\n",
    "##### Ù„Ø·ÙØ§ Ø§Ø¨ØªØ¯Ø§ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ :\n",
    "\n",
    "# Install Dependencies\n",
    "\n",
    "First, install the dependencies required for the project in your terminal. Use the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1: Loading and Preprocessing the Data\n",
    "\n",
    "In this step, the data is loaded from a CSV file and preprocessed. This process includes converting text to lowercase, removing punctuation, and filtering out stopwords.\n",
    "\n",
    "---\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ Ø´Ø§Ù…Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©ØŒ Ø­Ø°Ù Ù†Ø´Ø§Ù†Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ùˆ ØªÙˆÙ‚Ùâ€ŒÚ©Ù„Ù…Ø§Øª Ø§Ø³Øª.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Convert text to lowercase, remove punctuation, and remove stopwords \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\" Load data and convert ratings to sentiment labels \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    def rating_to_sentiment(rating):\n",
    "        try:\n",
    "            numeric_rating = float(re.sub(r'[^\\d.]', '', str(rating))) \n",
    "            return 1 if numeric_rating > 3 else 0  # Threshold of 3\n",
    "        except:\n",
    "            return 0  \n",
    "\n",
    "    df['sentiment'] = df['rating'].apply(rating_to_sentiment)\n",
    "    df['processed_review'] = df['review'].apply(preprocess_text)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Text Vectorization\n",
    "\n",
    "In this step, we use three different methods for vectorizing the text into numerical representations:\n",
    "- **TF-IDF**\n",
    "- **Word2Vec**\n",
    "- **BERT**\n",
    "\n",
    "---\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Ø¨Ø±Ø¯Ø§Ø± Ø³Ø§Ø²ÛŒ Ù…ØªÙ†\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ø§Ø² Ø³Ù‡ Ø±ÙˆØ´ Ø¨Ø±Ø¯Ø§Ø± Ø³Ø§Ø²ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:\n",
    "- **TF-IDF**\n",
    "- **Word2Vec**\n",
    "- **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def vectorize_text(texts, method='tfidf'):\n",
    "    \"\"\" Convert text to numerical representations using TF-IDF, Word2Vec, or BERT \"\"\"\n",
    "    if method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        return vectorizer.fit_transform(texts), vectorizer\n",
    "\n",
    "    elif method == 'word2vec':\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        w2v_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "        vectors = np.array([\n",
    "            np.mean([w2v_model.wv[word] for word in words if word in w2v_model.wv] or [np.zeros(100)], axis=0)\n",
    "            for words in tokenized_texts\n",
    "        ])\n",
    "        return vectors, w2v_model\n",
    "\n",
    "    elif method == 'bert':\n",
    "        bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        return bert_model.encode(texts.tolist(), show_progress_bar=True), bert_model\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorization method. Please choose 'tfidf', 'word2vec', or 'bert'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training Models with Best Hyperparameters\n",
    "\n",
    "In this step, we train several models such as **Logistic Regression**, **Random Forest**, and **K-Nearest Neighbors** using grid search to find the best hyperparameters. The models are evaluated based on their **F1 score**, and if the F1 score is below 0.8, the model is retrained.\n",
    "\n",
    "---\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Ø³ÙˆÙ…: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ú†Ù†Ø¯ÛŒÙ† Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù Ù…Ø§Ù†Ù†Ø¯ **Logistic Regression**ØŒ **Random Forest** Ùˆ **K-Nearest Neighbors** Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨Ú©Ù‡â€ŒØ§ÛŒ (Grid Search) Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² **F1** Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø§Ù…ØªÛŒØ§Ø² F1 Ø¨Ù‡ Ø¨Ø§Ù„Ø§ÛŒ Û°.Û¸ Ù†Ø±Ø³Ø¯ØŒ Ù…Ø¯Ù„ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def train_models_with_f1_requirement(X_train, X_test, y_train, y_test, min_f1=0.8):\n",
    "    \"\"\" Train models with hyperparameter tuning and ensure F1 score is above 0.8 \"\"\"\n",
    "    models = {\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'max_iter': [5000],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear']\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'knn': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_results = None\n",
    "\n",
    "    for model_name, details in models.items():\n",
    "        print(f\"\\nTraining {model_name} model...\")\n",
    "\n",
    "        while True:\n",
    "            grid_search = GridSearchCV(details['model'], details['params'], cv=5, scoring='f1', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            optimal_model = grid_search.best_estimator_\n",
    "            y_pred = optimal_model.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            print(f\"Model: {model_name} | Best F1: {f1:.4f} | Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "            if f1 >= min_f1:\n",
    "                break  \n",
    "\n",
    "        if f1 > best_score:\n",
    "            best_model = model_name\n",
    "            best_score = f1\n",
    "            best_results = {\n",
    "                'model': optimal_model,\n",
    "                'params': grid_search.best_params_,\n",
    "                'f1_score': f1,\n",
    "                'classification_report': classification_report(y_test, y_pred)\n",
    "            }\n",
    "\n",
    "    return best_model, best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate HTML Report from Best Model Results\n",
    "\n",
    "In this step, the results of the best model, including the F1 score and classification report, are converted into an HTML format for easy viewing.\n",
    "\n",
    "---\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ Ú†Ù‡Ø§Ø±Ù…: Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ HTML Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„\n",
    "\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø§Ù…ØªÛŒØ§Ø² F1 Ùˆ Ú¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ ÙØ±Ù…Øª HTML ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ø¨ØªÙˆØ§Ù†ÛŒÙ… Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒÙ….\n",
    " Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ Ø§Ú† ØªÛŒ Ø§Ù… Ø§Ù„ Ø±Ø§ Ø®Ø§Ø±Ø¬ Ø§Ø² Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„Ø± ÛŒØ§ ÙˆÛŒ Ø§Ø³ Ú©Ø¯ Ùˆ ... Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Ø®ÙˆØ¯ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø³Ú©Ø´Ù† Ù¾Ø§ÛŒÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ± Ù‡Ø§ Ùˆ Ø¯Ù‚Øª Ù‡Ø± Ù…Ø¯Ù„ Ùˆ Ù…ØªÙˆØ¯ Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ÙØ§ÛŒÙ„ Ø§Ú† ØªÛŒ Ø§Ù… Ø§Ù„ ØµØ±ÙØ§ Ø¬Ù†Ø¨Ù‡ Ø²ÛŒØ¨Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯. Ø¨Ø§ ØªØ´Ú©Ø± Ø§Ø² ØªÙˆØ¬Ù‡ Ø´Ù…Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m vector_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m X_vectorized, vectorizer \u001b[38;5;241m=\u001b[39m vectorize_text(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_review\u001b[39m\u001b[38;5;124m'\u001b[39m], method\u001b[38;5;241m=\u001b[39mvector_method)\n\u001b[1;32m----> 6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_vectorized, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m best_model_name, best_model_info \u001b[38;5;241m=\u001b[39m train_models_with_f1_requirement(X_train, X_test, y_train, y_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”¥ Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "data = load_and_clean_data('train_sentiment.csv')\n",
    "\n",
    "vector_method = 'tfidf'\n",
    "X_vectorized, vectorizer = vectorize_text(data['processed_review'], method=vector_method)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, data['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "best_model_name, best_model_info = train_models_with_f1_requirement(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"\\nðŸ”¥ Best Model: {best_model_name}\")\n",
    "print(f\"ðŸ”¹ F1 Score: {best_model_info['f1_score']:.4f}\")\n",
    "print(\"ðŸ”¹ Classification Report:\\n\", best_model_info['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML report saved as 'model_results.html'. Open this file to view the results.\n"
     ]
    }
   ],
   "source": [
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Best Model Results</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            background-color: #ffffff;\n",
    "            color: #333;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "        }}\n",
    "        .container {{\n",
    "            width: 80%;\n",
    "            margin: 50px auto;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            background-color: #f4f4f4;\n",
    "            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);\n",
    "        }}\n",
    "        h1 {{\n",
    "            text-align: center;\n",
    "            color: #2c3e50;\n",
    "        }}\n",
    "        .result-table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 30px 0;\n",
    "        }}\n",
    "        .result-table th, .result-table td {{\n",
    "            padding: 12px;\n",
    "            text-align: left;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "        }}\n",
    "        .result-table th {{\n",
    "            background-color: #3498db;\n",
    "            color: white;\n",
    "        }}\n",
    "        .result-table tr:hover {{\n",
    "            background-color: #f2f2f2;\n",
    "        }}\n",
    "        .classification-report {{\n",
    "            margin-top: 30px;\n",
    "            padding: 15px;\n",
    "            border: 1px solid #ddd;\n",
    "            background-color: #eaf1f9;\n",
    "            border-radius: 8px;\n",
    "        }}\n",
    "        .classification-report h2 {{\n",
    "            text-align: center;\n",
    "            color: #3498db;\n",
    "        }}\n",
    "        .classification-report pre {{\n",
    "            background-color: #fff;\n",
    "            padding: 20px;\n",
    "            border-radius: 5px;\n",
    "            white-space: pre-wrap;\n",
    "            word-wrap: break-word;\n",
    "            color: #2c3e50;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Best Performing Model</h1>\n",
    "        <table class=\"result-table\">\n",
    "            <tr>\n",
    "                <th>Metric</th>\n",
    "                <th>Value</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>ðŸ”¥ Best Model</td>\n",
    "                <td>{best_model_name}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>âœ… F1 Score</td>\n",
    "                <td>{best_model_info['f1_score']:.4f}</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "\n",
    "        <div class=\"classification-report\">\n",
    "            <h2>ðŸ“Š Classification Report:</h2>\n",
    "            <pre>{best_model_info['classification_report']}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "with open(\"model_results.html\", \"w\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"HTML report saved as 'model_results.html'. Open this file to view the results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
